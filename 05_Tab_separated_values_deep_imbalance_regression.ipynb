{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJc7GRtFbrRK"
      },
      "source": [
        "# **Deep Imbalanced Regression (DIR) Tutorial**\n",
        "\n",
        "In this notebook, we provide a hands-on tutorial for DIR on a small-scale dataset, [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), as a quick overview on how to perform practical (deep) imbalanced regression on custom datasets. Since the dataset is small, the training will be very fast and could be done on CPU. We show only the application of Label Distribution Smoothing (LDS); for FDS implementation, please refer to the main repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7mBaSzabN6I"
      },
      "source": [
        "# Download the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "xWuPbiEYmzvS",
        "outputId": "990f603a-8d3b-44b3-97ee-bf02a9b049b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n! pip install gdown\\n\\nimport gdown\\nimport os\\n\\nif not (os.path.exists(\\'./housing.data.train\\') and os.path.exists(\\'./housing.data.val\\') and os.path.exists(\\'./housing.data.test\\')):\\n    gdown.download(\"https://drive.google.com/file/d/1Bh2iJ3QsBsGNUsAmQersX6PQ2CEh923m/view?usp=sharing\", output=\\'./housing.data.train\\', quiet=False)\\n    gdown.download(\"https://drive.google.com/file/d/1ocj3Lm0Lbu_9NcV8RpqGv-7V8FE2Cg6h/view?usp=sharing\", output=\\'./housing.data.val\\', quiet=False)\\n    gdown.download(\"https://drive.google.com/file/d/1Q24s2GWx2D2M86wTyHVHRthcb3ibDuka/view?usp=sharing\", output=\\'./housing.data.test\\', quiet=False)\\nelse:\\n    print(\\'Data already downloaded.\\')\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "! pip install gdown\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "if not (os.path.exists('./housing.data.train') and os.path.exists('./housing.data.val') and os.path.exists('./housing.data.test')):\n",
        "    gdown.download(\"https://drive.google.com/file/d/1Bh2iJ3QsBsGNUsAmQersX6PQ2CEh923m/view?usp=sharing\", output='./housing.data.train', quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/file/d/1ocj3Lm0Lbu_9NcV8RpqGv-7V8FE2Cg6h/view?usp=sharing\", output='./housing.data.val', quiet=False)\n",
        "    gdown.download(\"https://drive.google.com/file/d/1Q24s2GWx2D2M86wTyHVHRthcb3ibDuka/view?usp=sharing\", output='./housing.data.test', quiet=False)\n",
        "else:\n",
        "    print('Data already downloaded.')\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnWsY4FzYC0Y"
      },
      "source": [
        "# Define the deep learning (neural network) model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jKm3tqsMhDOA"
      },
      "outputs": [],
      "source": [
        "# fcnet.py: Define the deep learning (neural network) model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FCNet(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, dropout=None):\n",
        "        super(FCNet, self).__init__()\n",
        "        self.fc0 = nn.Linear(13, layers[0])\n",
        "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
        "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
        "        self.fc_final = nn.Linear(layers[-1], 1)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_dropout = True if dropout else False\n",
        "        if self.use_dropout:\n",
        "            print(f'Using dropout: {dropout}')\n",
        "            self.dropout0 = nn.Dropout(p=dropout)\n",
        "            self.dropout1 = nn.Dropout(p=dropout)\n",
        "            self.dropout2 = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout0 = nn.Identity()\n",
        "            self.dropout1 = nn.Identity()\n",
        "            self.dropout2 = nn.Identity()\n",
        "\n",
        "    def forward(self, x, targets=None, epoch=None):\n",
        "        x = self.dropout0(F.relu(self.fc0(x)))\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        x = self.fc_final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def fcnet1(**kwargs):\n",
        "    return FCNet([256, 256, 256], **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWohJXZPYbY3"
      },
      "source": [
        "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Kzc8PTjhDIH"
      },
      "outputs": [],
      "source": [
        "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def weighted_mse_loss(inputs, targets, weights=None):\n",
        "    loss = (inputs - targets) ** 2\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def weighted_l1_loss(inputs, targets, weights=None):\n",
        "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def weighted_focal_mse_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
        "    loss = (inputs - targets) ** 2\n",
        "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
        "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def weighted_focal_l1_loss(inputs, targets, weights=None, activate='sigmoid', beta=.2, gamma=1):\n",
        "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
        "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
        "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "def weighted_huber_loss(inputs, targets, weights=None, beta=1.):\n",
        "    l1_loss = torch.abs(inputs - targets)\n",
        "    cond = l1_loss < beta\n",
        "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
        "    if weights is not None:\n",
        "        loss *= weights.expand_as(loss)\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aunbes6NY_Pz"
      },
      "source": [
        "# Define some utility functions (not the focus of this course)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f3XXH8Yik4M9"
      },
      "outputs": [],
      "source": [
        "# utils.py: Define some utility functions (not the focus of this course).\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal.windows import triang\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_batch_fmtstr(num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "def query_yes_no(question):\n",
        "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
        "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
        "    prompt = \" [Y/n] \"\n",
        "\n",
        "    while True:\n",
        "        print(question + prompt, end=':')\n",
        "        choice = input().lower()\n",
        "        if choice == '':\n",
        "            return valid['y']\n",
        "        elif choice in valid:\n",
        "            return valid[choice]\n",
        "        else:\n",
        "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
        "\n",
        "def prepare_folders(args):\n",
        "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
        "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
        "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
        "            shutil.rmtree(folders_util[-1])\n",
        "            print(folders_util[-1] + ' removed.')\n",
        "        else:\n",
        "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
        "    for folder in folders_util:\n",
        "        if not os.path.exists(folder):\n",
        "            print(f\"===> Creating folder: {folder}\")\n",
        "            os.mkdir(folder)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    lr = args.lr\n",
        "    for milestone in args.schedule:\n",
        "        lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def save_checkpoint(args, state, is_best, prefix=''):\n",
        "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        print(\"===> Saving current best checkpoint...\")\n",
        "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
        "\n",
        "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
        "    if torch.sum(v1) < 1e-10:\n",
        "        return matrix\n",
        "    if (v1 == 0.).any():\n",
        "        valid = (v1 != 0.)\n",
        "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
        "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
        "        return matrix\n",
        "\n",
        "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
        "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
        "\n",
        "def get_lds_kernel_window(kernel, ks, sigma):\n",
        "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
        "    half_ks = (ks - 1) // 2\n",
        "    if kernel == 'gaussian':\n",
        "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
        "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
        "    elif kernel == 'triang':\n",
        "        kernel_window = triang(ks)\n",
        "    else:\n",
        "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
        "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
        "\n",
        "    return kernel_window\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpbjtnotZUkk"
      },
      "source": [
        "# Define the data iterator (data loader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gqK8H1UghC_v"
      },
      "outputs": [],
      "source": [
        "# datasets.py: Define the data iterator (data loader).\n",
        "from scipy.ndimage import convolve1d\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "# from utils import get_lds_kernel_window\n",
        "\n",
        "class BostonHousing(data.Dataset):\n",
        "    def __init__(self, data_dir, split='train', reweight='none',\n",
        "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
        "        self.split = split\n",
        "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
        "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index % self.data.shape[0]\n",
        "        feature = self.data[index, :-1]\n",
        "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
        "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
        "        return feature, label, weight\n",
        "\n",
        "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
        "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
        "        assert reweight != 'none' if lds else True, \\\n",
        "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
        "\n",
        "        value_dict = {x: 0 for x in range(max_target)}\n",
        "        labels = self.data[:, -1].tolist()\n",
        "        # mbr\n",
        "        for label in labels:\n",
        "            value_dict[min(max_target - 1, int(label))] += 1\n",
        "        if reweight == 'sqrt_inv':\n",
        "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
        "        elif reweight == 'inverse':\n",
        "            value_dict = {k: np.clip(v, 5, 1000) for k, v in value_dict.items()}  # clip weights for inverse re-weight\n",
        "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
        "        if not len(num_per_label) or reweight == 'none':\n",
        "            return None\n",
        "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
        "\n",
        "        if lds:\n",
        "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
        "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
        "            smoothed_value = convolve1d(\n",
        "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
        "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
        "\n",
        "        weights = [np.float32(1 / x) for x in num_per_label]\n",
        "        scaling = len(weights) / np.sum(weights)\n",
        "        weights = [scaling * x for x in weights]\n",
        "        return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcuwqp5paPk3"
      },
      "source": [
        "# Set up some default configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T7mtQacPhBoW"
      },
      "outputs": [],
      "source": [
        "# train.py, Part 1: Set up some default configurations.\n",
        "import time\n",
        "import argparse\n",
        "#import logging\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from scipy.stats import gmean\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from resnet import resnet50\n",
        "# from fcnet import fcnet1\n",
        "# from loss import *\n",
        "# from datasets import BostonHousing\n",
        "# from utils import *\n",
        "\n",
        "import os\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
        "\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "# CPU only\n",
        "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
        "# imbalanced related\n",
        "# LDS\n",
        "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
        "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
        "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
        "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
        "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
        "\n",
        "# re-weighting: SQRT_INV / INV\n",
        "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
        "\n",
        "# training/optimization related\n",
        "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
        "parser.add_argument('--data_dir', type=str, default='./housing.data', help='data directory')\n",
        "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
        "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
        "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
        "parser.add_argument('--gpu', type=int, default=None)\n",
        "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
        "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
        "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
        "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
        "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
        "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
        "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
        "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
        "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
        "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
        "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
        "# checkpoints\n",
        "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
        "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
        "\n",
        "parser.set_defaults(augment=True)\n",
        "args, unknown = parser.parse_known_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nWvzFoYJnwuF"
      },
      "outputs": [],
      "source": [
        "args.cpu_only = True # Use CPU to train/test models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtG61vP8ROkP"
      },
      "source": [
        "# Train 4 different models using the 4 options below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4Wv3tQjBp0dG"
      },
      "outputs": [],
      "source": [
        "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
        "args.reweight = 'none'\n",
        "args.lds = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jYS6r4nyp36_"
      },
      "outputs": [],
      "source": [
        "# Option 2: To train the inverse weighting model:\n",
        "args.reweight = 'inverse'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fTYBWlx4p6R3"
      },
      "outputs": [],
      "source": [
        "# Option 3: To train the sqrt_inverse weighting model:\n",
        "args.reweight = 'sqrt_inv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W2j6V3UEqSoP"
      },
      "outputs": [],
      "source": [
        "# Option 4: To train the Label Distribution Smoothing (LDS) model:\n",
        "args.reweight = 'sqrt_inv'\n",
        "args.lds = True\n",
        "args.lds_kernel = 'gaussian'\n",
        "args.lds_ks = 5 # 5\n",
        "args.lds_sigma = 2 # 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nBTkrzbRbjT"
      },
      "source": [
        "# If you do not want to re-train models, download some trained models and directly use the models to predict labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "2Hbe77zCNJAr",
        "outputId": "4e1ff3ee-8ee1-470c-a67f-d03e1567fe51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nimport zipfile\\nimport os\\n\\nif not os.path.exists(\\'./trained_models\\'):\\n    # Download trained models (a zip file)\\n    gdown.download(\"https://drive.google.com/uc?export=download&id=1dY3m_LdZvCLNOU1HnjjpVO9Fa8-qc2vk\", output=\\'./trained_models.zip\\', quiet=False)\\n    # Unzip the file\\n    print(\\'Extracting downloaded models...\\')\\n    with zipfile.ZipFile(\\'./trained_models.zip\\') as zip_ref:\\n        zip_ref.extractall(\\'.\\')\\n    os.remove(\\'./trained_models.zip\\')\\n    print(\"Completed!\")\\nelse:\\n    print(\\'Trained models already exist. No need to download them.\\')\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "if not os.path.exists('./trained_models'):\n",
        "    # Download trained models (a zip file)\n",
        "    gdown.download(\"https://drive.google.com/uc?export=download&id=1dY3m_LdZvCLNOU1HnjjpVO9Fa8-qc2vk\", output='./trained_models.zip', quiet=False)\n",
        "    # Unzip the file\n",
        "    print('Extracting downloaded models...')\n",
        "    with zipfile.ZipFile('./trained_models.zip') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    os.remove('./trained_models.zip')\n",
        "    print(\"Completed!\")\n",
        "else:\n",
        "    print('Trained models already exist. No need to download them.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WYMK-mUPGe6"
      },
      "source": [
        "# Once you downloaded the models, you could configure to evaluate the trained models without re-training. Again we have 4 options, corresponding to 4 different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lAPDCoCGOxKr",
        "outputId": "6e903c89-fbf3-44f8-820b-0d1bdd18cda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n# Option 1: Evaluate the basic model\\nargs.evaluate = True\\nargs.resume = './trained_models/ckpt.base.pth.tar'\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "# Option 1: Evaluate the basic model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.base.pth.tar'\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JxcaG4dFPO8v",
        "outputId": "86615f4f-db03-4e9b-8bbb-b858d6fd2e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n# Option 2: Evaluate the inverse weighting model\\nargs.evaluate = True\\nargs.resume = './trained_models/ckpt.inverse.pth.tar'\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "# Option 2: Evaluate the inverse weighting model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.inverse.pth.tar'\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lV45Fm6FPTsp",
        "outputId": "0f28d10f-7e22-4a5b-ef33-42222dbade31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n# Option 3: Evaluate the sqrt_inverse weighting model\\nargs.evaluate = True\\nargs.resume = './trained_models/ckpt.sqrt_inv.pth.tar'\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Option 3: Evaluate the sqrt_inverse weighting model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.sqrt_inv.pth.tar'\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9onIhlctPf4K",
        "outputId": "5d24c3e5-0b85-4c56-c633-ea7a4ddeb5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n# Option 4: Evaluate the Label Distribution Smoothing (LDS) model\\nargs.evaluate = True\\nargs.resume = './trained_models/ckpt.lds.pth.tar'\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Option 4: Evaluate the Label Distribution Smoothing (LDS) model\n",
        "args.evaluate = True\n",
        "args.resume = './trained_models/ckpt.lds.pth.tar'\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-zCfrN5av1A"
      },
      "source": [
        "# Train/Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSrzhog1gxyY",
        "outputId": "7f2bd28d-2a68-44ee-b308-0a4a09b37061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Creating folder: checkpoint\n",
            "===> Creating folder: checkpoint/bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
            "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, cpu_only=True, data_dir='./housing.data', dataset='bostonhousing', epoch=10, evaluate=False, gpu=None, img_size=224, lds=True, lds_kernel='gaussian', lds_ks=5, lds_sigma=2, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='', reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
            "Store name: bostonhousing_fcnet1_lds_gau_5_2_adam_l1_0.001_64\n",
            "=====> Preparing data...\n",
            "Using re-weighting: [SQRT_INV]\n",
            "Using LDS: [GAUSSIAN] (5/2)\n",
            "Training data size: 23786\n",
            "Validation data size: 7929\n",
            "Test data size: 7929\n",
            "=====> Building model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][  0/372]\tTime  11.93 ( 11.93)\tData 11.8450 (11.8450)\tLoss (L1) 2946.923 (2946.923)\n",
            "Epoch: [0][ 10/372]\tTime   0.01 (  1.09)\tData 0.0062 (1.0784)\tLoss (L1) 3589.397 (5258.122)\n",
            "Epoch: [0][ 20/372]\tTime   0.01 (  0.58)\tData 0.0003 (0.5656)\tLoss (L1) 8686.631 (4440.580)\n",
            "Epoch: [0][ 30/372]\tTime   0.01 (  0.39)\tData 0.0003 (0.3834)\tLoss (L1) 1773.421 (3932.484)\n",
            "Epoch: [0][ 40/372]\tTime   0.01 (  0.30)\tData 0.0002 (0.2900)\tLoss (L1) 2161.477 (3685.221)\n",
            "Epoch: [0][ 50/372]\tTime   0.01 (  0.24)\tData 0.0001 (0.2332)\tLoss (L1) 4988.652 (3522.436)\n",
            "Epoch: [0][ 60/372]\tTime   0.01 (  0.20)\tData 0.0002 (0.1950)\tLoss (L1) 4376.665 (3723.913)\n",
            "Epoch: [0][ 70/372]\tTime   0.01 (  0.17)\tData 0.0003 (0.1676)\tLoss (L1) 1609.451 (3678.101)\n",
            "Epoch: [0][ 80/372]\tTime   0.01 (  0.15)\tData 0.0001 (0.1470)\tLoss (L1) 2607.310 (3527.469)\n",
            "Epoch: [0][ 90/372]\tTime   0.00 (  0.14)\tData 0.0001 (0.1309)\tLoss (L1) 2169.004 (3443.415)\n",
            "Epoch: [0][100/372]\tTime   0.01 (  0.12)\tData 0.0003 (0.1179)\tLoss (L1) 1795.239 (3310.409)\n",
            "Epoch: [0][110/372]\tTime   0.00 (  0.11)\tData 0.0003 (0.1073)\tLoss (L1) 2473.581 (3269.138)\n",
            "Epoch: [0][120/372]\tTime   0.01 (  0.10)\tData 0.0008 (0.0985)\tLoss (L1) 1129.760 (3215.554)\n",
            "Epoch: [0][130/372]\tTime   0.00 (  0.10)\tData 0.0002 (0.0910)\tLoss (L1) 3350.571 (3151.693)\n",
            "Epoch: [0][140/372]\tTime   0.01 (  0.09)\tData 0.0002 (0.0846)\tLoss (L1) 2047.788 (3104.342)\n",
            "Epoch: [0][150/372]\tTime   0.01 (  0.09)\tData 0.0001 (0.0790)\tLoss (L1) 1531.129 (3047.343)\n",
            "Epoch: [0][160/372]\tTime   0.01 (  0.08)\tData 0.0002 (0.0741)\tLoss (L1) 1873.927 (3054.655)\n",
            "Epoch: [0][170/372]\tTime   0.01 (  0.08)\tData 0.0002 (0.0698)\tLoss (L1) 2781.416 (2988.687)\n",
            "Epoch: [0][180/372]\tTime   0.01 (  0.07)\tData 0.0016 (0.0660)\tLoss (L1) 1409.134 (2956.138)\n",
            "Epoch: [0][190/372]\tTime   0.00 (  0.07)\tData 0.0001 (0.0626)\tLoss (L1) 2401.828 (2949.149)\n",
            "Epoch: [0][200/372]\tTime   0.00 (  0.07)\tData 0.0001 (0.0595)\tLoss (L1) 1163.096 (2899.709)\n",
            "Epoch: [0][210/372]\tTime   0.00 (  0.06)\tData 0.0002 (0.0567)\tLoss (L1) 964.049 (2857.549)\n",
            "Epoch: [0][220/372]\tTime   0.00 (  0.06)\tData 0.0003 (0.0541)\tLoss (L1) 3347.918 (2826.794)\n",
            "Epoch: [0][230/372]\tTime   0.01 (  0.06)\tData 0.0002 (0.0518)\tLoss (L1) 1679.362 (2850.285)\n",
            "Epoch: [0][240/372]\tTime   0.01 (  0.06)\tData 0.0001 (0.0497)\tLoss (L1) 2249.513 (2840.489)\n",
            "Epoch: [0][250/372]\tTime   0.01 (  0.05)\tData 0.0003 (0.0477)\tLoss (L1) 1969.000 (2841.652)\n",
            "Epoch: [0][260/372]\tTime   0.01 (  0.05)\tData 0.0002 (0.0459)\tLoss (L1) 1284.584 (2802.891)\n",
            "Epoch: [0][270/372]\tTime   0.01 (  0.05)\tData 0.0004 (0.0442)\tLoss (L1) 1717.234 (2805.869)\n",
            "Epoch: [0][280/372]\tTime   0.00 (  0.05)\tData 0.0002 (0.0426)\tLoss (L1) 3040.814 (2790.658)\n",
            "Epoch: [0][290/372]\tTime   0.00 (  0.05)\tData 0.0001 (0.0412)\tLoss (L1) 2116.574 (2782.258)\n",
            "Epoch: [0][300/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0398)\tLoss (L1) 2916.924 (2777.481)\n",
            "Epoch: [0][310/372]\tTime   0.00 (  0.04)\tData 0.0001 (0.0386)\tLoss (L1) 4531.694 (2777.235)\n",
            "Epoch: [0][320/372]\tTime   0.00 (  0.04)\tData 0.0000 (0.0374)\tLoss (L1) 2693.071 (2755.016)\n",
            "Epoch: [0][330/372]\tTime   0.00 (  0.04)\tData 0.0000 (0.0362)\tLoss (L1) 2307.738 (2742.738)\n",
            "Epoch: [0][340/372]\tTime   0.00 (  0.04)\tData 0.0000 (0.0352)\tLoss (L1) 1336.651 (2736.908)\n",
            "Epoch: [0][350/372]\tTime   0.00 (  0.04)\tData 0.0001 (0.0342)\tLoss (L1) 1712.115 (2717.504)\n",
            "Epoch: [0][360/372]\tTime   0.00 (  0.04)\tData 0.0001 (0.0332)\tLoss (L1) 1921.269 (2701.172)\n",
            "Epoch: [0][370/372]\tTime   0.00 (  0.04)\tData 0.0001 (0.0323)\tLoss (L1) 2476.212 (2689.546)\n",
            "Val: [  0/124]\tTime  1.113 ( 1.113)\tLoss (MSE) 4218304.000 (4218304.000)\tLoss (L1) 1313.373 (1313.373)\n",
            "Val: [ 10/124]\tTime  0.002 ( 0.105)\tLoss (MSE) 27401086.000 (52920575.205)\tLoss (L1) 1772.668 (2351.160)\n",
            "Val: [ 20/124]\tTime  0.006 ( 0.056)\tLoss (MSE) 26555458.000 (369785204.774)\tLoss (L1) 2238.667 (2748.861)\n",
            "Val: [ 30/124]\tTime  0.001 ( 0.039)\tLoss (MSE) 72436536.000 (359185563.444)\tLoss (L1) 2913.165 (2861.627)\n",
            "Val: [ 40/124]\tTime  0.004 ( 0.031)\tLoss (MSE) 13162560.000 (279442208.921)\tLoss (L1) 1727.532 (2685.849)\n",
            "Val: [ 50/124]\tTime  0.005 ( 0.026)\tLoss (MSE) 102002944.000 (237951192.485)\tLoss (L1) 3865.880 (2658.349)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.022)\tLoss (MSE) 61607248.000 (217052365.635)\tLoss (L1) 2374.579 (2679.741)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.019)\tLoss (MSE) 704943872.000 (199949443.701)\tLoss (L1) 5515.356 (2658.289)\n",
            "Val: [ 80/124]\tTime  0.002 ( 0.017)\tLoss (MSE) 18798848.000 (210330470.775)\tLoss (L1) 1983.187 (2746.116)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.015)\tLoss (MSE) 60023688.000 (190372976.047)\tLoss (L1) 2274.419 (2678.930)\n",
            "Val: [100/124]\tTime  0.002 ( 0.014)\tLoss (MSE) 81096560.000 (176207487.750)\tLoss (L1) 3513.873 (2674.501)\n",
            "Val: [110/124]\tTime  0.002 ( 0.013)\tLoss (MSE) 6598836.000 (171412806.061)\tLoss (L1) 1439.110 (2668.158)\n",
            "Val: [120/124]\tTime  0.001 ( 0.012)\tLoss (MSE) 33682812.000 (161739626.280)\tLoss (L1) 2631.255 (2637.209)\n",
            " * Overall: MSE 159102436.902\tL1 2636.369\tG-Mean 755.114\n",
            " * Many: MSE 6011665.612\tL1 1436.423\tG-Mean 660.423\n",
            " * Median: MSE 89700570.700\tL1 5007.195\tG-Mean 1276.936\n",
            " * Low: MSE 5893024314.204\tL1 33564.363\tG-Mean 6817.340\n",
            "Best L1 Loss: 2636.369\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #0: Train loss [2721.5098]; Val loss: MSE [159102436.9015], L1 [2636.3687], G-Mean [755.1144]\n",
            "Epoch: [1][  0/372]\tTime   1.16 (  1.16)\tData 1.1134 (1.1134)\tLoss (L1) 1961.917 (1961.917)\n",
            "Epoch: [1][ 10/372]\tTime   0.01 (  0.11)\tData 0.0001 (0.1025)\tLoss (L1) 1536.917 (2461.595)\n",
            "Epoch: [1][ 20/372]\tTime   0.01 (  0.06)\tData 0.0001 (0.0538)\tLoss (L1) 1966.058 (2227.180)\n",
            "Epoch: [1][ 30/372]\tTime   0.01 (  0.04)\tData 0.0002 (0.0365)\tLoss (L1) 1923.115 (2213.816)\n",
            "Epoch: [1][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0278)\tLoss (L1) 2715.125 (2307.367)\n",
            "Epoch: [1][ 50/372]\tTime   0.00 (  0.03)\tData 0.0001 (0.0225)\tLoss (L1) 2262.244 (2270.071)\n",
            "Epoch: [1][ 60/372]\tTime   0.00 (  0.03)\tData 0.0001 (0.0189)\tLoss (L1) 2967.813 (2283.364)\n",
            "Epoch: [1][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0163)\tLoss (L1) 1862.131 (2237.790)\n",
            "Epoch: [1][ 80/372]\tTime   0.00 (  0.02)\tData 0.0002 (0.0143)\tLoss (L1) 1640.740 (2250.451)\n",
            "Epoch: [1][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0128)\tLoss (L1) 2462.342 (2436.709)\n",
            "Epoch: [1][100/372]\tTime   0.01 (  0.02)\tData 0.0003 (0.0116)\tLoss (L1) 1521.910 (2502.134)\n",
            "Epoch: [1][110/372]\tTime   0.01 (  0.02)\tData 0.0004 (0.0106)\tLoss (L1) 2742.850 (2469.976)\n",
            "Epoch: [1][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0098)\tLoss (L1) 3358.498 (2461.517)\n",
            "Epoch: [1][130/372]\tTime   0.01 (  0.02)\tData 0.0003 (0.0091)\tLoss (L1) 1608.946 (2458.935)\n",
            "Epoch: [1][140/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0085)\tLoss (L1) 2861.243 (2442.736)\n",
            "Epoch: [1][150/372]\tTime   0.00 (  0.01)\tData 0.0002 (0.0079)\tLoss (L1) 2161.670 (2434.896)\n",
            "Epoch: [1][160/372]\tTime   0.01 (  0.01)\tData 0.0003 (0.0075)\tLoss (L1) 1430.730 (2435.638)\n",
            "Epoch: [1][170/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0070)\tLoss (L1) 1998.314 (2563.109)\n",
            "Epoch: [1][180/372]\tTime   0.01 (  0.01)\tData 0.0002 (0.0067)\tLoss (L1) 2698.155 (2554.420)\n",
            "Epoch: [1][190/372]\tTime   0.01 (  0.01)\tData 0.0007 (0.0064)\tLoss (L1) 2331.027 (2518.641)\n",
            "Epoch: [1][200/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0061)\tLoss (L1) 1333.630 (2511.501)\n",
            "Epoch: [1][210/372]\tTime   0.01 (  0.01)\tData 0.0046 (0.0059)\tLoss (L1) 1649.020 (2494.810)\n",
            "Epoch: [1][220/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 2315.627 (2477.536)\n",
            "Epoch: [1][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0054)\tLoss (L1) 1636.667 (2482.152)\n",
            "Epoch: [1][240/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0052)\tLoss (L1) 2824.428 (2478.439)\n",
            "Epoch: [1][250/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0050)\tLoss (L1) 1856.563 (2457.586)\n",
            "Epoch: [1][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0049)\tLoss (L1) 1798.523 (2461.117)\n",
            "Epoch: [1][270/372]\tTime   0.00 (  0.01)\tData 0.0002 (0.0047)\tLoss (L1) 3778.506 (2472.920)\n",
            "Epoch: [1][280/372]\tTime   0.01 (  0.01)\tData 0.0003 (0.0045)\tLoss (L1) 1585.540 (2471.625)\n",
            "Epoch: [1][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 1734.915 (2469.734)\n",
            "Epoch: [1][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 2386.684 (2472.963)\n",
            "Epoch: [1][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 3139.902 (2468.374)\n",
            "Epoch: [1][320/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0040)\tLoss (L1) 1827.053 (2478.169)\n",
            "Epoch: [1][330/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0039)\tLoss (L1) 1676.862 (2472.039)\n",
            "Epoch: [1][340/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0038)\tLoss (L1) 2148.711 (2460.796)\n",
            "Epoch: [1][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 1531.152 (2452.414)\n",
            "Epoch: [1][360/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0036)\tLoss (L1) 2483.277 (2451.576)\n",
            "Epoch: [1][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0035)\tLoss (L1) 2453.471 (2471.618)\n",
            "Val: [  0/124]\tTime  1.069 ( 1.069)\tLoss (MSE) 7235602.000 (7235602.000)\tLoss (L1) 2052.733 (2052.733)\n",
            "Val: [ 10/124]\tTime  0.002 ( 0.104)\tLoss (MSE) 35386620.000 (60327643.818)\tLoss (L1) 2647.249 (3161.550)\n",
            "Val: [ 20/124]\tTime  0.005 ( 0.056)\tLoss (MSE) 31994258.000 (377984203.381)\tLoss (L1) 2889.835 (3550.461)\n",
            "Val: [ 30/124]\tTime  0.009 ( 0.040)\tLoss (MSE) 82711944.000 (368204590.968)\tLoss (L1) 3920.127 (3678.835)\n",
            "Val: [ 40/124]\tTime  0.005 ( 0.031)\tLoss (MSE) 18754558.000 (288040912.195)\tLoss (L1) 2577.833 (3509.752)\n",
            "Val: [ 50/124]\tTime  0.004 ( 0.026)\tLoss (MSE) 116714240.000 (246396687.039)\tLoss (L1) 4536.981 (3489.618)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.022)\tLoss (MSE) 67702232.000 (225463258.984)\tLoss (L1) 3235.642 (3512.650)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.019)\tLoss (MSE) 719245632.000 (208261022.225)\tLoss (L1) 6384.011 (3488.771)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.017)\tLoss (MSE) 27057640.000 (219241998.642)\tLoss (L1) 2920.352 (3573.750)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.015)\tLoss (MSE) 68640064.000 (199104542.538)\tLoss (L1) 3109.271 (3505.923)\n",
            "Val: [100/124]\tTime  0.002 ( 0.014)\tLoss (MSE) 92859360.000 (184917009.550)\tLoss (L1) 4224.150 (3502.092)\n",
            "Val: [110/124]\tTime  0.002 ( 0.013)\tLoss (MSE) 11004453.000 (180140142.356)\tLoss (L1) 2302.283 (3498.298)\n",
            "Val: [120/124]\tTime  0.002 ( 0.012)\tLoss (MSE) 41375772.000 (170285762.488)\tLoss (L1) 3421.735 (3461.843)\n",
            " * Overall: MSE 167654025.586\tL1 3462.260\tG-Mean 1720.106\n",
            " * Many: MSE 10369808.756\tL1 2335.040\tG-Mean 1735.346\n",
            " * Median: MSE 105756616.788\tL1 5340.700\tG-Mean 1384.365\n",
            " * Low: MSE 6013414207.281\tL1 34192.418\tG-Mean 3579.923\n",
            "Best L1 Loss: 2636.369\n",
            "Epoch #1: Train loss [2471.0916]; Val loss: MSE [167654025.5861], L1 [3462.2600], G-Mean [1720.1064]\n",
            "Epoch: [2][  0/372]\tTime   1.21 (  1.21)\tData 1.1711 (1.1711)\tLoss (L1) 2882.836 (2882.836)\n",
            "Epoch: [2][ 10/372]\tTime   0.01 (  0.12)\tData 0.0002 (0.1069)\tLoss (L1) 2769.269 (3109.435)\n",
            "Epoch: [2][ 20/372]\tTime   0.00 (  0.07)\tData 0.0002 (0.0563)\tLoss (L1) 3306.373 (3020.658)\n",
            "Epoch: [2][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0382)\tLoss (L1) 3264.522 (3328.276)\n",
            "Epoch: [2][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0289)\tLoss (L1) 6146.397 (3299.543)\n",
            "Epoch: [2][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0233)\tLoss (L1) 2010.052 (3098.861)\n",
            "Epoch: [2][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0195)\tLoss (L1) 2732.660 (2950.638)\n",
            "Epoch: [2][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0168)\tLoss (L1) 3635.520 (3059.996)\n",
            "Epoch: [2][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0147)\tLoss (L1) 1722.027 (3058.753)\n",
            "Epoch: [2][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0131)\tLoss (L1) 1660.496 (3032.067)\n",
            "Epoch: [2][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0119)\tLoss (L1) 1974.146 (3036.294)\n",
            "Epoch: [2][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0108)\tLoss (L1) 3068.482 (2974.558)\n",
            "Epoch: [2][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0099)\tLoss (L1) 2846.210 (2925.550)\n",
            "Epoch: [2][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0092)\tLoss (L1) 4257.482 (2916.156)\n",
            "Epoch: [2][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0086)\tLoss (L1) 2986.148 (2874.593)\n",
            "Epoch: [2][150/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0080)\tLoss (L1) 2490.955 (2912.460)\n",
            "Epoch: [2][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0075)\tLoss (L1) 1636.728 (2863.198)\n",
            "Epoch: [2][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0071)\tLoss (L1) 2564.689 (2902.619)\n",
            "Epoch: [2][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0067)\tLoss (L1) 3244.479 (2955.199)\n",
            "Epoch: [2][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0064)\tLoss (L1) 2562.342 (2919.938)\n",
            "Epoch: [2][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0061)\tLoss (L1) 2061.149 (2929.203)\n",
            "Epoch: [2][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0058)\tLoss (L1) 14476.461 (2970.511)\n",
            "Epoch: [2][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 1763.268 (2941.723)\n",
            "Epoch: [2][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0053)\tLoss (L1) 1492.467 (2947.059)\n",
            "Epoch: [2][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0051)\tLoss (L1) 2630.691 (2917.290)\n",
            "Epoch: [2][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0049)\tLoss (L1) 2381.005 (2891.880)\n",
            "Epoch: [2][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0047)\tLoss (L1) 1722.622 (2841.612)\n",
            "Epoch: [2][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 1511.311 (2814.068)\n",
            "Epoch: [2][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 1574.793 (2769.974)\n",
            "Epoch: [2][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 2224.903 (2757.977)\n",
            "Epoch: [2][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0042)\tLoss (L1) 2485.841 (2743.712)\n",
            "Epoch: [2][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 2201.711 (2726.324)\n",
            "Epoch: [2][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 1366.576 (2704.451)\n",
            "Epoch: [2][330/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 3335.365 (2699.637)\n",
            "Epoch: [2][340/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 1498.594 (2678.605)\n",
            "Epoch: [2][350/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0036)\tLoss (L1) 1139.794 (2671.327)\n",
            "Epoch: [2][360/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0035)\tLoss (L1) 2207.498 (2647.176)\n",
            "Epoch: [2][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0034)\tLoss (L1) 1817.944 (2626.435)\n",
            "Val: [  0/124]\tTime  1.117 ( 1.117)\tLoss (MSE) 4797945.000 (4797945.000)\tLoss (L1) 1375.514 (1375.514)\n",
            "Val: [ 10/124]\tTime  0.004 ( 0.105)\tLoss (MSE) 27773418.000 (53461212.409)\tLoss (L1) 1719.049 (2356.814)\n",
            "Val: [ 20/124]\tTime  0.001 ( 0.057)\tLoss (MSE) 27067156.000 (370274246.333)\tLoss (L1) 2195.150 (2749.517)\n",
            "Val: [ 30/124]\tTime  0.004 ( 0.041)\tLoss (MSE) 73000896.000 (360102514.984)\tLoss (L1) 3001.834 (2862.009)\n",
            "Val: [ 40/124]\tTime  0.003 ( 0.031)\tLoss (MSE) 13925184.000 (280370085.012)\tLoss (L1) 1709.345 (2688.606)\n",
            "Val: [ 50/124]\tTime  0.001 ( 0.026)\tLoss (MSE) 103146288.000 (238996116.657)\tLoss (L1) 3821.088 (2665.693)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.022)\tLoss (MSE) 62608852.000 (218049170.697)\tLoss (L1) 2438.373 (2688.573)\n",
            "Val: [ 70/124]\tTime  0.002 ( 0.019)\tLoss (MSE) 705959680.000 (200906606.725)\tLoss (L1) 5558.096 (2663.278)\n",
            "Val: [ 80/124]\tTime  0.002 ( 0.017)\tLoss (MSE) 18840334.000 (211336171.562)\tLoss (L1) 1993.999 (2748.020)\n",
            "Val: [ 90/124]\tTime  0.002 ( 0.016)\tLoss (MSE) 63161512.000 (191389813.797)\tLoss (L1) 2296.576 (2679.151)\n",
            "Val: [100/124]\tTime  0.002 ( 0.014)\tLoss (MSE) 81149168.000 (177213077.530)\tLoss (L1) 3409.977 (2673.264)\n",
            "Val: [110/124]\tTime  0.001 ( 0.013)\tLoss (MSE) 7293020.500 (172430279.252)\tLoss (L1) 1531.021 (2668.824)\n",
            "Val: [120/124]\tTime  0.001 ( 0.012)\tLoss (MSE) 35048404.000 (162676131.926)\tLoss (L1) 2613.750 (2634.344)\n",
            " * Overall: MSE 160033809.912\tL1 2633.372\tG-Mean 670.749\n",
            " * Many: MSE 6349941.699\tL1 1461.770\tG-Mean 631.897\n",
            " * Median: MSE 91584194.232\tL1 4806.940\tG-Mean 681.409\n",
            " * Low: MSE 5910297991.935\tL1 33510.002\tG-Mean 5101.722\n",
            "Best L1 Loss: 2633.372\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #2: Train loss [2624.6676]; Val loss: MSE [160033809.9120], L1 [2633.3716], G-Mean [670.7485]\n",
            "Epoch: [3][  0/372]\tTime   1.16 (  1.16)\tData 1.1245 (1.1245)\tLoss (L1) 1480.917 (1480.917)\n",
            "Epoch: [3][ 10/372]\tTime   0.01 (  0.11)\tData 0.0002 (0.1029)\tLoss (L1) 2358.991 (2817.336)\n",
            "Epoch: [3][ 20/372]\tTime   0.01 (  0.06)\tData 0.0001 (0.0540)\tLoss (L1) 2158.713 (2504.928)\n",
            "Epoch: [3][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0367)\tLoss (L1) 2038.771 (2340.421)\n",
            "Epoch: [3][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0278)\tLoss (L1) 3262.805 (2288.175)\n",
            "Epoch: [3][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0224)\tLoss (L1) 1473.098 (2185.666)\n",
            "Epoch: [3][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0187)\tLoss (L1) 1724.143 (2210.986)\n",
            "Epoch: [3][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0161)\tLoss (L1) 2554.268 (2232.841)\n",
            "Epoch: [3][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0141)\tLoss (L1) 3110.739 (2280.929)\n",
            "Epoch: [3][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0126)\tLoss (L1) 3284.508 (2305.076)\n",
            "Epoch: [3][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0114)\tLoss (L1) 2057.812 (2393.890)\n",
            "Epoch: [3][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0104)\tLoss (L1) 2038.068 (2444.388)\n",
            "Epoch: [3][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0096)\tLoss (L1) 2196.772 (2410.691)\n",
            "Epoch: [3][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0089)\tLoss (L1) 1646.784 (2372.985)\n",
            "Epoch: [3][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0083)\tLoss (L1) 1402.949 (2356.061)\n",
            "Epoch: [3][150/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0078)\tLoss (L1) 1541.678 (2366.776)\n",
            "Epoch: [3][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0073)\tLoss (L1) 2991.908 (2344.337)\n",
            "Epoch: [3][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0069)\tLoss (L1) 3430.154 (2357.337)\n",
            "Epoch: [3][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0065)\tLoss (L1) 1853.455 (2334.006)\n",
            "Epoch: [3][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0062)\tLoss (L1) 5285.590 (2331.370)\n",
            "Epoch: [3][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0059)\tLoss (L1) 2394.381 (2341.388)\n",
            "Epoch: [3][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 1600.005 (2339.975)\n",
            "Epoch: [3][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0054)\tLoss (L1) 2455.436 (2325.453)\n",
            "Epoch: [3][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0051)\tLoss (L1) 4961.906 (2355.081)\n",
            "Epoch: [3][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0049)\tLoss (L1) 1795.686 (2352.767)\n",
            "Epoch: [3][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0048)\tLoss (L1) 1537.123 (2357.754)\n",
            "Epoch: [3][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 2137.000 (2347.733)\n",
            "Epoch: [3][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 1559.741 (2339.176)\n",
            "Epoch: [3][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 1528.013 (2321.657)\n",
            "Epoch: [3][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 4507.623 (2315.998)\n",
            "Epoch: [3][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0040)\tLoss (L1) 1416.241 (2354.168)\n",
            "Epoch: [3][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 3242.178 (2352.495)\n",
            "Epoch: [3][320/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0038)\tLoss (L1) 2386.323 (2382.186)\n",
            "Epoch: [3][330/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 2419.254 (2399.314)\n",
            "Epoch: [3][340/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0036)\tLoss (L1) 2303.990 (2411.644)\n",
            "Epoch: [3][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0035)\tLoss (L1) 2443.646 (2423.095)\n",
            "Epoch: [3][360/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0034)\tLoss (L1) 2040.824 (2440.303)\n",
            "Epoch: [3][370/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0033)\tLoss (L1) 1742.525 (2460.340)\n",
            "Val: [  0/124]\tTime  1.124 ( 1.124)\tLoss (MSE) 5915886.500 (5915886.500)\tLoss (L1) 1487.111 (1487.111)\n",
            "Val: [ 10/124]\tTime  0.001 ( 0.106)\tLoss (MSE) 25704916.000 (51107225.386)\tLoss (L1) 1727.966 (2399.737)\n",
            "Val: [ 20/124]\tTime  0.002 ( 0.057)\tLoss (MSE) 25882638.000 (366783124.202)\tLoss (L1) 2294.150 (2762.151)\n",
            "Val: [ 30/124]\tTime  0.004 ( 0.041)\tLoss (MSE) 67844808.000 (356402977.363)\tLoss (L1) 2918.186 (2850.933)\n",
            "Val: [ 40/124]\tTime  0.004 ( 0.032)\tLoss (MSE) 12542941.000 (277036997.250)\tLoss (L1) 1719.781 (2676.768)\n",
            "Val: [ 50/124]\tTime  0.001 ( 0.026)\tLoss (MSE) 95993584.000 (235856780.466)\tLoss (L1) 3968.886 (2657.814)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.022)\tLoss (MSE) 60776932.000 (214962655.734)\tLoss (L1) 2434.529 (2678.897)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.019)\tLoss (MSE) 699678848.000 (197882339.475)\tLoss (L1) 5504.375 (2650.762)\n",
            "Val: [ 80/124]\tTime  0.002 ( 0.017)\tLoss (MSE) 14943115.000 (207984850.886)\tLoss (L1) 1837.971 (2736.263)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.016)\tLoss (MSE) 60673412.000 (188166499.420)\tLoss (L1) 2276.081 (2666.641)\n",
            "Val: [100/124]\tTime  0.001 ( 0.014)\tLoss (MSE) 74439016.000 (173971363.047)\tLoss (L1) 3277.451 (2653.338)\n",
            "Val: [110/124]\tTime  0.001 ( 0.013)\tLoss (MSE) 8641607.000 (169124900.818)\tLoss (L1) 1397.363 (2646.978)\n",
            "Val: [120/124]\tTime  0.004 ( 0.012)\tLoss (MSE) 32761448.000 (159471066.682)\tLoss (L1) 2626.659 (2618.233)\n",
            " * Overall: MSE 156833361.411\tL1 2617.308\tG-Mean 792.718\n",
            " * Many: MSE 5949277.920\tL1 1425.768\tG-Mean 668.470\n",
            " * Median: MSE 84348696.742\tL1 5019.255\tG-Mean 1726.908\n",
            " * Low: MSE 5827738068.789\tL1 33099.177\tG-Mean 7670.670\n",
            "Best L1 Loss: 2617.308\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #3: Train loss [2458.3921]; Val loss: MSE [156833361.4105], L1 [2617.3082], G-Mean [792.7183]\n",
            "Epoch: [4][  0/372]\tTime   1.21 (  1.21)\tData 1.1931 (1.1931)\tLoss (L1) 4899.074 (4899.074)\n",
            "Epoch: [4][ 10/372]\tTime   0.01 (  0.12)\tData 0.0001 (0.1091)\tLoss (L1) 1449.193 (2488.945)\n",
            "Epoch: [4][ 20/372]\tTime   0.01 (  0.07)\tData 0.0001 (0.0572)\tLoss (L1) 2205.985 (2617.584)\n",
            "Epoch: [4][ 30/372]\tTime   0.01 (  0.05)\tData 0.0002 (0.0389)\tLoss (L1) 2767.307 (2732.573)\n",
            "Epoch: [4][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0295)\tLoss (L1) 1817.675 (2567.514)\n",
            "Epoch: [4][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0237)\tLoss (L1) 1494.433 (2442.362)\n",
            "Epoch: [4][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0199)\tLoss (L1) 1887.547 (2595.389)\n",
            "Epoch: [4][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0171)\tLoss (L1) 1768.033 (2537.241)\n",
            "Epoch: [4][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0150)\tLoss (L1) 3355.884 (2549.232)\n",
            "Epoch: [4][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0134)\tLoss (L1) 1995.763 (2538.183)\n",
            "Epoch: [4][100/372]\tTime   0.01 (  0.02)\tData 0.0018 (0.0121)\tLoss (L1) 2508.970 (2507.111)\n",
            "Epoch: [4][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0110)\tLoss (L1) 2335.183 (2482.990)\n",
            "Epoch: [4][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0101)\tLoss (L1) 2412.282 (2490.262)\n",
            "Epoch: [4][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0094)\tLoss (L1) 1184.800 (2563.840)\n",
            "Epoch: [4][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0087)\tLoss (L1) 1565.984 (2542.495)\n",
            "Epoch: [4][150/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0082)\tLoss (L1) 1481.785 (2492.561)\n",
            "Epoch: [4][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0077)\tLoss (L1) 1117.865 (2503.215)\n",
            "Epoch: [4][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0072)\tLoss (L1) 2144.160 (2472.254)\n",
            "Epoch: [4][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0068)\tLoss (L1) 1280.295 (2446.729)\n",
            "Epoch: [4][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0065)\tLoss (L1) 1911.902 (2440.452)\n",
            "Epoch: [4][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0062)\tLoss (L1) 3130.457 (2444.789)\n",
            "Epoch: [4][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0059)\tLoss (L1) 1886.843 (2432.726)\n",
            "Epoch: [4][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 2506.843 (2431.094)\n",
            "Epoch: [4][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0054)\tLoss (L1) 3546.176 (2427.762)\n",
            "Epoch: [4][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0052)\tLoss (L1) 2161.100 (2414.025)\n",
            "Epoch: [4][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0050)\tLoss (L1) 1651.317 (2410.775)\n",
            "Epoch: [4][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0048)\tLoss (L1) 4330.572 (2405.781)\n",
            "Epoch: [4][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0047)\tLoss (L1) 1375.890 (2400.733)\n",
            "Epoch: [4][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0045)\tLoss (L1) 1380.138 (2399.592)\n",
            "Epoch: [4][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 1401.323 (2383.849)\n",
            "Epoch: [4][300/372]\tTime   0.01 (  0.01)\tData 0.0002 (0.0042)\tLoss (L1) 1818.249 (2380.647)\n",
            "Epoch: [4][310/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0041)\tLoss (L1) 2837.823 (2373.469)\n",
            "Epoch: [4][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0040)\tLoss (L1) 1320.844 (2371.558)\n",
            "Epoch: [4][330/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 1821.034 (2367.494)\n",
            "Epoch: [4][340/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 4178.117 (2355.475)\n",
            "Epoch: [4][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0036)\tLoss (L1) 1696.734 (2390.482)\n",
            "Epoch: [4][360/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0035)\tLoss (L1) 2464.665 (2390.764)\n",
            "Epoch: [4][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0034)\tLoss (L1) 2114.362 (2393.019)\n",
            "Val: [  0/124]\tTime  1.115 ( 1.115)\tLoss (MSE) 5679025.000 (5679025.000)\tLoss (L1) 1658.005 (1658.005)\n",
            "Val: [ 10/124]\tTime  0.004 ( 0.105)\tLoss (MSE) 30702302.000 (56105546.455)\tLoss (L1) 2064.097 (2713.748)\n",
            "Val: [ 20/124]\tTime  0.004 ( 0.056)\tLoss (MSE) 29109842.000 (373426984.571)\tLoss (L1) 2468.867 (3127.134)\n",
            "Val: [ 30/124]\tTime  0.009 ( 0.040)\tLoss (MSE) 75811152.000 (363732018.823)\tLoss (L1) 3390.051 (3244.236)\n",
            "Val: [ 40/124]\tTime  0.002 ( 0.031)\tLoss (MSE) 16267807.000 (283794400.963)\tLoss (L1) 2178.196 (3069.691)\n",
            "Val: [ 50/124]\tTime  0.004 ( 0.026)\tLoss (MSE) 109174032.000 (242432803.735)\tLoss (L1) 4035.610 (3040.691)\n",
            "Val: [ 60/124]\tTime  0.002 ( 0.022)\tLoss (MSE) 65114628.000 (221454956.287)\tLoss (L1) 2838.398 (3065.982)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.019)\tLoss (MSE) 710907264.000 (204260366.585)\tLoss (L1) 5955.663 (3040.668)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.017)\tLoss (MSE) 21520886.000 (214988970.148)\tLoss (L1) 2445.139 (3128.488)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.015)\tLoss (MSE) 67163152.000 (194959689.385)\tLoss (L1) 2706.326 (3060.054)\n",
            "Val: [100/124]\tTime  0.002 ( 0.014)\tLoss (MSE) 86045936.000 (180775292.243)\tLoss (L1) 3796.775 (3054.009)\n",
            "Val: [110/124]\tTime  0.001 ( 0.013)\tLoss (MSE) 9484100.000 (176019188.599)\tLoss (L1) 2034.121 (3051.950)\n",
            "Val: [120/124]\tTime  0.002 ( 0.012)\tLoss (MSE) 38019624.000 (166312133.764)\tLoss (L1) 2970.147 (3020.470)\n",
            " * Overall: MSE 163665324.808\tL1 3021.307\tG-Mean 1100.440\n",
            " * Many: MSE 8218679.304\tL1 1872.623\tG-Mean 1078.931\n",
            " * Median: MSE 98003736.860\tL1 5043.017\tG-Mean 989.083\n",
            " * Low: MSE 5962705338.206\tL1 33819.748\tG-Mean 3687.611\n",
            "Best L1 Loss: 2617.308\n",
            "Epoch #4: Train loss [2392.0001]; Val loss: MSE [163665324.8077], L1 [3021.3074], G-Mean [1100.4397]\n",
            "Epoch: [5][  0/372]\tTime   1.18 (  1.18)\tData 1.1032 (1.1032)\tLoss (L1) 2673.771 (2673.771)\n",
            "Epoch: [5][ 10/372]\tTime   0.01 (  0.12)\tData 0.0002 (0.1008)\tLoss (L1) 3933.524 (2897.775)\n",
            "Epoch: [5][ 20/372]\tTime   0.01 (  0.07)\tData 0.0003 (0.0530)\tLoss (L1) 2176.935 (2791.306)\n",
            "Epoch: [5][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0361)\tLoss (L1) 4882.758 (2737.756)\n",
            "Epoch: [5][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0273)\tLoss (L1) 1496.940 (2667.748)\n",
            "Epoch: [5][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0220)\tLoss (L1) 4349.090 (2678.692)\n",
            "Epoch: [5][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0185)\tLoss (L1) 2545.430 (2584.686)\n",
            "Epoch: [5][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0160)\tLoss (L1) 1931.142 (2527.146)\n",
            "Epoch: [5][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0140)\tLoss (L1) 1572.854 (2583.660)\n",
            "Epoch: [5][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0125)\tLoss (L1) 1598.885 (2560.267)\n",
            "Epoch: [5][100/372]\tTime   0.01 (  0.02)\tData 0.0002 (0.0113)\tLoss (L1) 1313.736 (2576.384)\n",
            "Epoch: [5][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0103)\tLoss (L1) 1387.264 (2560.070)\n",
            "Epoch: [5][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0095)\tLoss (L1) 1839.643 (2552.735)\n",
            "Epoch: [5][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0087)\tLoss (L1) 1830.719 (2511.549)\n",
            "Epoch: [5][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0081)\tLoss (L1) 2802.008 (2491.359)\n",
            "Epoch: [5][150/372]\tTime   0.01 (  0.01)\tData 0.0002 (0.0076)\tLoss (L1) 2630.265 (2471.476)\n",
            "Epoch: [5][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0072)\tLoss (L1) 2061.385 (2473.357)\n",
            "Epoch: [5][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0067)\tLoss (L1) 1920.655 (2448.606)\n",
            "Epoch: [5][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0064)\tLoss (L1) 1899.980 (2425.351)\n",
            "Epoch: [5][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0061)\tLoss (L1) 1036.529 (2407.613)\n",
            "Epoch: [5][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0058)\tLoss (L1) 2490.316 (2457.033)\n",
            "Epoch: [5][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0055)\tLoss (L1) 724.874 (2429.346)\n",
            "Epoch: [5][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0053)\tLoss (L1) 1472.448 (2397.528)\n",
            "Epoch: [5][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0051)\tLoss (L1) 2994.292 (2384.181)\n",
            "Epoch: [5][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0049)\tLoss (L1) 4938.912 (2415.660)\n",
            "Epoch: [5][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0047)\tLoss (L1) 2701.080 (2433.713)\n",
            "Epoch: [5][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0045)\tLoss (L1) 13143.744 (2483.610)\n",
            "Epoch: [5][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 2827.944 (2498.290)\n",
            "Epoch: [5][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0042)\tLoss (L1) 2548.715 (2505.097)\n",
            "Epoch: [5][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 1558.703 (2480.402)\n",
            "Epoch: [5][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 1094.134 (2479.421)\n",
            "Epoch: [5][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 1135.630 (2461.884)\n",
            "Epoch: [5][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 3230.517 (2453.897)\n",
            "Epoch: [5][330/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0036)\tLoss (L1) 4287.323 (2451.443)\n",
            "Epoch: [5][340/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0035)\tLoss (L1) 1857.793 (2443.761)\n",
            "Epoch: [5][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0034)\tLoss (L1) 2041.587 (2435.002)\n",
            "Epoch: [5][360/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0033)\tLoss (L1) 1723.434 (2434.629)\n",
            "Epoch: [5][370/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0032)\tLoss (L1) 2357.801 (2432.172)\n",
            "Val: [  0/124]\tTime  1.128 ( 1.128)\tLoss (MSE) 4215006.500 (4215006.500)\tLoss (L1) 1251.736 (1251.736)\n",
            "Val: [ 10/124]\tTime  0.004 ( 0.106)\tLoss (MSE) 26995794.000 (51956439.909)\tLoss (L1) 1717.976 (2263.748)\n",
            "Val: [ 20/124]\tTime  0.006 ( 0.058)\tLoss (MSE) 25791266.000 (368699638.762)\tLoss (L1) 2110.435 (2656.424)\n",
            "Val: [ 30/124]\tTime  0.006 ( 0.041)\tLoss (MSE) 70935056.000 (358296499.468)\tLoss (L1) 2863.776 (2762.762)\n",
            "Val: [ 40/124]\tTime  0.002 ( 0.032)\tLoss (MSE) 12848766.000 (278679967.037)\tLoss (L1) 1631.335 (2589.250)\n",
            "Val: [ 50/124]\tTime  0.004 ( 0.026)\tLoss (MSE) 102313336.000 (237303995.755)\tLoss (L1) 3799.656 (2564.226)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.022)\tLoss (MSE) 61426348.000 (216372923.877)\tLoss (L1) 2351.600 (2586.148)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.019)\tLoss (MSE) 702183936.000 (199272287.063)\tLoss (L1) 5431.007 (2558.628)\n",
            "Val: [ 80/124]\tTime  0.002 ( 0.017)\tLoss (MSE) 17311978.000 (209676853.728)\tLoss (L1) 1839.106 (2644.740)\n",
            "Val: [ 90/124]\tTime  0.002 ( 0.015)\tLoss (MSE) 61277432.000 (189764635.654)\tLoss (L1) 2154.388 (2576.270)\n",
            "Val: [100/124]\tTime  0.001 ( 0.014)\tLoss (MSE) 80570848.000 (175601017.797)\tLoss (L1) 3375.249 (2571.754)\n",
            "Val: [110/124]\tTime  0.002 ( 0.013)\tLoss (MSE) 6538067.500 (170781842.910)\tLoss (L1) 1389.526 (2565.318)\n",
            "Val: [120/124]\tTime  0.002 ( 0.012)\tLoss (MSE) 33293398.000 (161068634.510)\tLoss (L1) 2495.519 (2532.165)\n",
            " * Overall: MSE 158419779.313\tL1 2531.269\tG-Mean 656.211\n",
            " * Many: MSE 5546218.141\tL1 1327.693\tG-Mean 561.693\n",
            " * Median: MSE 88405121.747\tL1 4921.258\tG-Mean 1259.225\n",
            " * Low: MSE 5887626144.181\tL1 33495.175\tG-Mean 6896.072\n",
            "Best L1 Loss: 2531.269\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #5: Train loss [2431.3318]; Val loss: MSE [158419779.3134], L1 [2531.2690], G-Mean [656.2106]\n",
            "Epoch: [6][  0/372]\tTime   1.17 (  1.17)\tData 1.1222 (1.1222)\tLoss (L1) 2636.490 (2636.490)\n",
            "Epoch: [6][ 10/372]\tTime   0.01 (  0.12)\tData 0.0001 (0.1026)\tLoss (L1) 2392.269 (2499.673)\n",
            "Epoch: [6][ 20/372]\tTime   0.01 (  0.06)\tData 0.0001 (0.0539)\tLoss (L1) 1787.648 (2372.542)\n",
            "Epoch: [6][ 30/372]\tTime   0.01 (  0.05)\tData 0.0003 (0.0366)\tLoss (L1) 3242.611 (2323.684)\n",
            "Epoch: [6][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0277)\tLoss (L1) 2579.955 (2400.019)\n",
            "Epoch: [6][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0223)\tLoss (L1) 1126.541 (2288.103)\n",
            "Epoch: [6][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0187)\tLoss (L1) 1411.266 (2184.193)\n",
            "Epoch: [6][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0161)\tLoss (L1) 1491.879 (2288.043)\n",
            "Epoch: [6][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0141)\tLoss (L1) 2113.973 (2271.501)\n",
            "Epoch: [6][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0126)\tLoss (L1) 1880.099 (2253.204)\n",
            "Epoch: [6][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0114)\tLoss (L1) 1610.453 (2210.835)\n",
            "Epoch: [6][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0104)\tLoss (L1) 3082.032 (2249.295)\n",
            "Epoch: [6][120/372]\tTime   0.01 (  0.02)\tData 0.0029 (0.0095)\tLoss (L1) 1588.479 (2250.106)\n",
            "Epoch: [6][130/372]\tTime   0.01 (  0.02)\tData 0.0007 (0.0088)\tLoss (L1) 2430.462 (2254.071)\n",
            "Epoch: [6][140/372]\tTime   0.01 (  0.02)\tData 0.0030 (0.0083)\tLoss (L1) 2014.270 (2232.565)\n",
            "Epoch: [6][150/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0078)\tLoss (L1) 2475.976 (2243.712)\n",
            "Epoch: [6][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0073)\tLoss (L1) 1788.623 (2264.299)\n",
            "Epoch: [6][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0069)\tLoss (L1) 2936.924 (2319.670)\n",
            "Epoch: [6][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0065)\tLoss (L1) 1462.011 (2291.230)\n",
            "Epoch: [6][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0062)\tLoss (L1) 1271.344 (2355.422)\n",
            "Epoch: [6][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0059)\tLoss (L1) 2843.056 (2382.849)\n",
            "Epoch: [6][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0057)\tLoss (L1) 2148.612 (2395.254)\n",
            "Epoch: [6][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0054)\tLoss (L1) 3442.760 (2408.970)\n",
            "Epoch: [6][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0052)\tLoss (L1) 1085.916 (2428.072)\n",
            "Epoch: [6][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0050)\tLoss (L1) 2380.236 (2440.836)\n",
            "Epoch: [6][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0048)\tLoss (L1) 1549.101 (2416.298)\n",
            "Epoch: [6][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 2067.907 (2422.297)\n",
            "Epoch: [6][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0045)\tLoss (L1) 3532.085 (2410.033)\n",
            "Epoch: [6][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 2773.361 (2394.612)\n",
            "Epoch: [6][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0042)\tLoss (L1) 1144.750 (2386.100)\n",
            "Epoch: [6][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0040)\tLoss (L1) 3148.834 (2387.259)\n",
            "Epoch: [6][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 1720.490 (2378.927)\n",
            "Epoch: [6][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 3352.574 (2380.270)\n",
            "Epoch: [6][330/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 1576.093 (2363.491)\n",
            "Epoch: [6][340/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0036)\tLoss (L1) 2305.763 (2365.617)\n",
            "Epoch: [6][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0035)\tLoss (L1) 1454.746 (2359.672)\n",
            "Epoch: [6][360/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0034)\tLoss (L1) 2446.995 (2354.173)\n",
            "Epoch: [6][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0033)\tLoss (L1) 1746.616 (2387.213)\n",
            "Val: [  0/124]\tTime  1.302 ( 1.302)\tLoss (MSE) 6259999.000 (6259999.000)\tLoss (L1) 1778.081 (1778.081)\n",
            "Val: [ 10/124]\tTime  0.006 ( 0.125)\tLoss (MSE) 32710952.000 (57916359.864)\tLoss (L1) 2303.636 (2890.508)\n",
            "Val: [ 20/124]\tTime  0.004 ( 0.079)\tLoss (MSE) 30520538.000 (375396888.929)\tLoss (L1) 2639.031 (3310.563)\n",
            "Val: [ 30/124]\tTime  0.006 ( 0.055)\tLoss (MSE) 78993704.000 (365781730.274)\tLoss (L1) 3625.407 (3433.934)\n",
            "Val: [ 40/124]\tTime  0.001 ( 0.042)\tLoss (MSE) 17471802.000 (285745326.402)\tLoss (L1) 2384.086 (3262.899)\n",
            "Val: [ 50/124]\tTime  0.004 ( 0.035)\tLoss (MSE) 111324944.000 (244209412.814)\tLoss (L1) 4210.205 (3238.127)\n",
            "Val: [ 60/124]\tTime  0.002 ( 0.030)\tLoss (MSE) 66182220.000 (223222712.336)\tLoss (L1) 3001.677 (3261.931)\n",
            "Val: [ 70/124]\tTime  0.003 ( 0.026)\tLoss (MSE) 715104704.000 (206000079.021)\tLoss (L1) 6169.842 (3237.487)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.023)\tLoss (MSE) 24116328.000 (216816273.660)\tLoss (L1) 2660.402 (3324.238)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.020)\tLoss (MSE) 68064392.000 (196716806.192)\tLoss (L1) 2905.188 (3256.795)\n",
            "Val: [100/124]\tTime  0.000 ( 0.018)\tLoss (MSE) 88375040.000 (182536323.792)\tLoss (L1) 3993.085 (3251.355)\n",
            "Val: [110/124]\tTime  0.000 ( 0.017)\tLoss (MSE) 9775640.000 (177776642.333)\tLoss (L1) 2120.034 (3249.688)\n",
            "Val: [120/124]\tTime  0.001 ( 0.016)\tLoss (MSE) 39822196.000 (167945864.008)\tLoss (L1) 3195.247 (3214.273)\n",
            " * Overall: MSE 165311242.506\tL1 3215.695\tG-Mean 1355.779\n",
            " * Many: MSE 9067240.105\tL1 2078.959\tG-Mean 1364.418\n",
            " * Median: MSE 101401729.372\tL1 5157.935\tG-Mean 1053.069\n",
            " * Low: MSE 5984053509.332\tL1 33974.785\tG-Mean 3651.811\n",
            "Best L1 Loss: 2531.269\n",
            "Epoch #6: Train loss [2388.2370]; Val loss: MSE [165311242.5062], L1 [3215.6954], G-Mean [1355.7791]\n",
            "Epoch: [7][  0/372]\tTime   1.19 (  1.19)\tData 1.1162 (1.1162)\tLoss (L1) 2877.703 (2877.703)\n",
            "Epoch: [7][ 10/372]\tTime   0.01 (  0.12)\tData 0.0002 (0.1024)\tLoss (L1) 2356.283 (3200.237)\n",
            "Epoch: [7][ 20/372]\tTime   0.01 (  0.07)\tData 0.0001 (0.0537)\tLoss (L1) 1579.112 (2827.426)\n",
            "Epoch: [7][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0365)\tLoss (L1) 2488.247 (2661.891)\n",
            "Epoch: [7][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0278)\tLoss (L1) 1404.307 (2658.747)\n",
            "Epoch: [7][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0224)\tLoss (L1) 2120.783 (2611.675)\n",
            "Epoch: [7][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0188)\tLoss (L1) 1152.219 (2548.691)\n",
            "Epoch: [7][ 70/372]\tTime   0.01 (  0.02)\tData 0.0017 (0.0162)\tLoss (L1) 2759.059 (2441.635)\n",
            "Epoch: [7][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0143)\tLoss (L1) 1305.673 (2580.897)\n",
            "Epoch: [7][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0127)\tLoss (L1) 3066.227 (2511.564)\n",
            "Epoch: [7][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0115)\tLoss (L1) 2030.030 (2475.549)\n",
            "Epoch: [7][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0105)\tLoss (L1) 1419.478 (2444.849)\n",
            "Epoch: [7][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0096)\tLoss (L1) 1894.693 (2423.583)\n",
            "Epoch: [7][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0089)\tLoss (L1) 4620.256 (2420.891)\n",
            "Epoch: [7][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0083)\tLoss (L1) 1877.419 (2381.139)\n",
            "Epoch: [7][150/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0077)\tLoss (L1) 2999.737 (2358.802)\n",
            "Epoch: [7][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0073)\tLoss (L1) 2350.340 (2340.188)\n",
            "Epoch: [7][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0069)\tLoss (L1) 1259.914 (2319.107)\n",
            "Epoch: [7][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0065)\tLoss (L1) 4768.305 (2347.943)\n",
            "Epoch: [7][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0062)\tLoss (L1) 2248.004 (2346.673)\n",
            "Epoch: [7][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0059)\tLoss (L1) 2712.664 (2371.705)\n",
            "Epoch: [7][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 1554.869 (2380.089)\n",
            "Epoch: [7][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0053)\tLoss (L1) 14924.631 (2445.405)\n",
            "Epoch: [7][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0051)\tLoss (L1) 1338.338 (2426.958)\n",
            "Epoch: [7][240/372]\tTime   0.01 (  0.01)\tData 0.0002 (0.0049)\tLoss (L1) 2156.228 (2403.804)\n",
            "Epoch: [7][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0047)\tLoss (L1) 1782.904 (2408.032)\n",
            "Epoch: [7][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 5261.655 (2427.816)\n",
            "Epoch: [7][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 2011.100 (2417.650)\n",
            "Epoch: [7][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 1735.282 (2428.658)\n",
            "Epoch: [7][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 3327.682 (2425.382)\n",
            "Epoch: [7][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0040)\tLoss (L1) 2105.484 (2409.784)\n",
            "Epoch: [7][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 3119.723 (2438.391)\n",
            "Epoch: [7][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 1971.955 (2438.730)\n",
            "Epoch: [7][330/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0036)\tLoss (L1) 1889.907 (2424.008)\n",
            "Epoch: [7][340/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0035)\tLoss (L1) 3410.222 (2417.226)\n",
            "Epoch: [7][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0034)\tLoss (L1) 1450.161 (2409.285)\n",
            "Epoch: [7][360/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0034)\tLoss (L1) 2072.953 (2415.742)\n",
            "Epoch: [7][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0033)\tLoss (L1) 1858.999 (2401.741)\n",
            "Val: [  0/124]\tTime  1.154 ( 1.154)\tLoss (MSE) 3855069.500 (3855069.500)\tLoss (L1) 1219.367 (1219.367)\n",
            "Val: [ 10/124]\tTime  0.001 ( 0.109)\tLoss (MSE) 26657060.000 (51269108.682)\tLoss (L1) 1765.475 (2262.366)\n",
            "Val: [ 20/124]\tTime  0.003 ( 0.058)\tLoss (MSE) 25189110.000 (368009245.929)\tLoss (L1) 2100.896 (2655.054)\n",
            "Val: [ 30/124]\tTime  0.005 ( 0.041)\tLoss (MSE) 69442624.000 (357502857.129)\tLoss (L1) 2866.989 (2761.883)\n",
            "Val: [ 40/124]\tTime  0.001 ( 0.032)\tLoss (MSE) 12434349.000 (277926612.878)\tLoss (L1) 1641.035 (2586.443)\n",
            "Val: [ 50/124]\tTime  0.001 ( 0.027)\tLoss (MSE) 101841792.000 (236617308.245)\tLoss (L1) 3829.806 (2560.100)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.023)\tLoss (MSE) 60960872.000 (215696283.033)\tLoss (L1) 2359.349 (2583.034)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.020)\tLoss (MSE) 700427840.000 (198594160.704)\tLoss (L1) 5429.871 (2554.320)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.018)\tLoss (MSE) 16869460.000 (208934327.858)\tLoss (L1) 1840.823 (2639.239)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.016)\tLoss (MSE) 60401588.000 (189060042.962)\tLoss (L1) 2143.228 (2570.844)\n",
            "Val: [100/124]\tTime  0.001 ( 0.014)\tLoss (MSE) 79817776.000 (174903119.381)\tLoss (L1) 3411.905 (2566.674)\n",
            "Val: [110/124]\tTime  0.002 ( 0.013)\tLoss (MSE) 6123644.500 (170116793.937)\tLoss (L1) 1361.397 (2559.708)\n",
            "Val: [120/124]\tTime  0.001 ( 0.012)\tLoss (MSE) 32645604.000 (160406229.246)\tLoss (L1) 2510.408 (2526.972)\n",
            " * Overall: MSE 157754508.578\tL1 2526.534\tG-Mean 718.788\n",
            " * Many: MSE 5241104.793\tL1 1313.101\tG-Mean 604.038\n",
            " * Median: MSE 86856581.126\tL1 4988.111\tG-Mean 1576.875\n",
            " * Low: MSE 5878503714.245\tL1 33493.944\tG-Mean 7595.229\n",
            "Best L1 Loss: 2526.534\n",
            "===> Saving current best checkpoint...\n",
            "Epoch #7: Train loss [2399.9002]; Val loss: MSE [157754508.5776], L1 [2526.5339], G-Mean [718.7878]\n",
            "Epoch: [8][  0/372]\tTime   1.26 (  1.26)\tData 1.1880 (1.1880)\tLoss (L1) 1602.425 (1602.425)\n",
            "Epoch: [8][ 10/372]\tTime   0.02 (  0.13)\tData 0.0002 (0.1091)\tLoss (L1) 1825.032 (3536.864)\n",
            "Epoch: [8][ 20/372]\tTime   0.01 (  0.07)\tData 0.0001 (0.0573)\tLoss (L1) 2033.941 (2747.956)\n",
            "Epoch: [8][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0389)\tLoss (L1) 1849.062 (2941.800)\n",
            "Epoch: [8][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0294)\tLoss (L1) 1234.272 (2683.054)\n",
            "Epoch: [8][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0237)\tLoss (L1) 4021.586 (2660.824)\n",
            "Epoch: [8][ 60/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0200)\tLoss (L1) 3435.974 (2683.476)\n",
            "Epoch: [8][ 70/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0172)\tLoss (L1) 1534.031 (2575.168)\n",
            "Epoch: [8][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0151)\tLoss (L1) 2189.731 (2587.792)\n",
            "Epoch: [8][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0134)\tLoss (L1) 2042.943 (2578.131)\n",
            "Epoch: [8][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0121)\tLoss (L1) 3742.557 (2562.580)\n",
            "Epoch: [8][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0110)\tLoss (L1) 3009.212 (2535.726)\n",
            "Epoch: [8][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0102)\tLoss (L1) 1448.708 (2524.206)\n",
            "Epoch: [8][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0094)\tLoss (L1) 2354.820 (2488.782)\n",
            "Epoch: [8][140/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0088)\tLoss (L1) 3430.906 (2465.198)\n",
            "Epoch: [8][150/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0082)\tLoss (L1) 1411.171 (2449.183)\n",
            "Epoch: [8][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0077)\tLoss (L1) 2402.025 (2428.998)\n",
            "Epoch: [8][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0073)\tLoss (L1) 1700.108 (2448.997)\n",
            "Epoch: [8][180/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0069)\tLoss (L1) 3434.162 (2420.037)\n",
            "Epoch: [8][190/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0066)\tLoss (L1) 1585.931 (2410.296)\n",
            "Epoch: [8][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0062)\tLoss (L1) 2434.312 (2395.508)\n",
            "Epoch: [8][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0060)\tLoss (L1) 2951.241 (2366.854)\n",
            "Epoch: [8][220/372]\tTime   0.01 (  0.01)\tData 0.0015 (0.0057)\tLoss (L1) 2874.147 (2345.735)\n",
            "Epoch: [8][230/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0055)\tLoss (L1) 3556.110 (2364.309)\n",
            "Epoch: [8][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0053)\tLoss (L1) 1109.958 (2392.381)\n",
            "Epoch: [8][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0051)\tLoss (L1) 1487.950 (2367.201)\n",
            "Epoch: [8][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0049)\tLoss (L1) 1614.708 (2349.421)\n",
            "Epoch: [8][270/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0047)\tLoss (L1) 2790.584 (2366.589)\n",
            "Epoch: [8][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 2053.769 (2368.100)\n",
            "Epoch: [8][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 1191.704 (2358.578)\n",
            "Epoch: [8][300/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 1225.685 (2338.186)\n",
            "Epoch: [8][310/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0042)\tLoss (L1) 2574.278 (2352.325)\n",
            "Epoch: [8][320/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0040)\tLoss (L1) 1570.919 (2340.094)\n",
            "Epoch: [8][330/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0039)\tLoss (L1) 2459.140 (2336.449)\n",
            "Epoch: [8][340/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0038)\tLoss (L1) 2574.167 (2333.413)\n",
            "Epoch: [8][350/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 4194.696 (2358.954)\n",
            "Epoch: [8][360/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0036)\tLoss (L1) 2564.543 (2352.301)\n",
            "Epoch: [8][370/372]\tTime   0.00 (  0.01)\tData 0.0000 (0.0035)\tLoss (L1) 5221.976 (2356.627)\n",
            "Val: [  0/124]\tTime  1.165 ( 1.165)\tLoss (MSE) 5208218.500 (5208218.500)\tLoss (L1) 1516.962 (1516.962)\n",
            "Val: [ 10/124]\tTime  0.003 ( 0.110)\tLoss (MSE) 30797752.000 (55427595.864)\tLoss (L1) 1954.260 (2599.621)\n",
            "Val: [ 20/124]\tTime  0.003 ( 0.059)\tLoss (MSE) 28133762.000 (372564807.214)\tLoss (L1) 2342.115 (3004.415)\n",
            "Val: [ 30/124]\tTime  0.020 ( 0.042)\tLoss (MSE) 75542064.000 (362764502.048)\tLoss (L1) 3310.871 (3120.415)\n",
            "Val: [ 40/124]\tTime  0.001 ( 0.033)\tLoss (MSE) 15819637.000 (282909611.110)\tLoss (L1) 2115.322 (2947.719)\n",
            "Val: [ 50/124]\tTime  0.004 ( 0.027)\tLoss (MSE) 108759560.000 (241453931.422)\tLoss (L1) 3971.069 (2917.326)\n",
            "Val: [ 60/124]\tTime  0.001 ( 0.023)\tLoss (MSE) 64364364.000 (220479266.172)\tLoss (L1) 2695.219 (2941.217)\n",
            "Val: [ 70/124]\tTime  0.001 ( 0.020)\tLoss (MSE) 710516736.000 (203309882.852)\tLoss (L1) 5817.437 (2916.012)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.018)\tLoss (MSE) 21662962.000 (214014412.278)\tLoss (L1) 2341.696 (3002.218)\n",
            "Val: [ 90/124]\tTime  0.001 ( 0.016)\tLoss (MSE) 65962792.000 (193999785.610)\tLoss (L1) 2568.299 (2932.917)\n",
            "Val: [100/124]\tTime  0.002 ( 0.015)\tLoss (MSE) 85062424.000 (179842803.084)\tLoss (L1) 3683.276 (2928.048)\n",
            "Val: [110/124]\tTime  0.001 ( 0.013)\tLoss (MSE) 8439068.000 (175107326.086)\tLoss (L1) 1827.366 (2926.479)\n",
            "Val: [120/124]\tTime  0.003 ( 0.012)\tLoss (MSE) 37614872.000 (165319245.558)\tLoss (L1) 2875.723 (2891.714)\n",
            " * Overall: MSE 162677125.876\tL1 2893.117\tG-Mean 962.018\n",
            " * Many: MSE 7548883.217\tL1 1737.387\tG-Mean 943.566\n",
            " * Median: MSE 96918012.060\tL1 4957.723\tG-Mean 848.390\n",
            " * Low: MSE 5950954420.908\tL1 33733.804\tG-Mean 3486.025\n",
            "Best L1 Loss: 2526.534\n",
            "Epoch #8: Train loss [2355.7131]; Val loss: MSE [162677125.8762], L1 [2893.1166], G-Mean [962.0177]\n",
            "Epoch: [9][  0/372]\tTime   1.20 (  1.20)\tData 1.1687 (1.1687)\tLoss (L1) 1513.717 (1513.717)\n",
            "Epoch: [9][ 10/372]\tTime   0.01 (  0.12)\tData 0.0002 (0.1070)\tLoss (L1) 3048.643 (2249.353)\n",
            "Epoch: [9][ 20/372]\tTime   0.01 (  0.07)\tData 0.0001 (0.0562)\tLoss (L1) 2099.411 (2390.071)\n",
            "Epoch: [9][ 30/372]\tTime   0.01 (  0.05)\tData 0.0001 (0.0381)\tLoss (L1) 2203.266 (2205.971)\n",
            "Epoch: [9][ 40/372]\tTime   0.01 (  0.04)\tData 0.0001 (0.0289)\tLoss (L1) 1648.024 (2382.935)\n",
            "Epoch: [9][ 50/372]\tTime   0.01 (  0.03)\tData 0.0001 (0.0233)\tLoss (L1) 976.938 (2326.427)\n",
            "Epoch: [9][ 60/372]\tTime   0.01 (  0.03)\tData 0.0029 (0.0195)\tLoss (L1) 2477.942 (2394.870)\n",
            "Epoch: [9][ 70/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0168)\tLoss (L1) 1209.434 (2362.929)\n",
            "Epoch: [9][ 80/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0147)\tLoss (L1) 1621.723 (2278.648)\n",
            "Epoch: [9][ 90/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0131)\tLoss (L1) 1732.381 (2262.239)\n",
            "Epoch: [9][100/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0119)\tLoss (L1) 2373.186 (2233.173)\n",
            "Epoch: [9][110/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0109)\tLoss (L1) 5019.109 (2275.179)\n",
            "Epoch: [9][120/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0100)\tLoss (L1) 1843.792 (2358.775)\n",
            "Epoch: [9][130/372]\tTime   0.01 (  0.02)\tData 0.0001 (0.0092)\tLoss (L1) 2685.029 (2370.032)\n",
            "Epoch: [9][140/372]\tTime   0.01 (  0.02)\tData 0.0002 (0.0086)\tLoss (L1) 2112.542 (2407.115)\n",
            "Epoch: [9][150/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0080)\tLoss (L1) 1005.979 (2366.220)\n",
            "Epoch: [9][160/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0075)\tLoss (L1) 3014.149 (2408.824)\n",
            "Epoch: [9][170/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0071)\tLoss (L1) 1663.298 (2397.017)\n",
            "Epoch: [9][180/372]\tTime   0.01 (  0.01)\tData 0.0002 (0.0067)\tLoss (L1) 1767.307 (2415.276)\n",
            "Epoch: [9][190/372]\tTime   0.01 (  0.01)\tData 0.0035 (0.0064)\tLoss (L1) 1913.487 (2402.046)\n",
            "Epoch: [9][200/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0061)\tLoss (L1) 2276.627 (2413.127)\n",
            "Epoch: [9][210/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0058)\tLoss (L1) 1173.173 (2401.122)\n",
            "Epoch: [9][220/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0056)\tLoss (L1) 2847.781 (2401.893)\n",
            "Epoch: [9][230/372]\tTime   0.01 (  0.01)\tData 0.0003 (0.0054)\tLoss (L1) 3778.076 (2403.199)\n",
            "Epoch: [9][240/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0052)\tLoss (L1) 2037.384 (2459.375)\n",
            "Epoch: [9][250/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0050)\tLoss (L1) 3364.178 (2446.952)\n",
            "Epoch: [9][260/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0048)\tLoss (L1) 2819.161 (2435.141)\n",
            "Epoch: [9][270/372]\tTime   0.01 (  0.01)\tData 0.0035 (0.0048)\tLoss (L1) 1946.614 (2424.318)\n",
            "Epoch: [9][280/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0046)\tLoss (L1) 2123.203 (2431.311)\n",
            "Epoch: [9][290/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0045)\tLoss (L1) 1256.991 (2417.604)\n",
            "Epoch: [9][300/372]\tTime   0.01 (  0.01)\tData 0.0046 (0.0045)\tLoss (L1) 3860.993 (2424.604)\n",
            "Epoch: [9][310/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0044)\tLoss (L1) 1460.646 (2423.117)\n",
            "Epoch: [9][320/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0043)\tLoss (L1) 2535.706 (2405.886)\n",
            "Epoch: [9][330/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0041)\tLoss (L1) 3881.781 (2402.255)\n",
            "Epoch: [9][340/372]\tTime   0.01 (  0.01)\tData 0.0003 (0.0040)\tLoss (L1) 2233.315 (2406.003)\n",
            "Epoch: [9][350/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0039)\tLoss (L1) 2225.250 (2384.847)\n",
            "Epoch: [9][360/372]\tTime   0.01 (  0.01)\tData 0.0001 (0.0038)\tLoss (L1) 2659.071 (2376.024)\n",
            "Epoch: [9][370/372]\tTime   0.00 (  0.01)\tData 0.0001 (0.0037)\tLoss (L1) 1420.435 (2361.660)\n",
            "Val: [  0/124]\tTime  1.204 ( 1.204)\tLoss (MSE) 3920908.500 (3920908.500)\tLoss (L1) 1227.913 (1227.913)\n",
            "Val: [ 10/124]\tTime  0.001 ( 0.113)\tLoss (MSE) 27160844.000 (51861405.614)\tLoss (L1) 1686.946 (2267.484)\n",
            "Val: [ 20/124]\tTime  0.004 ( 0.060)\tLoss (MSE) 25745152.000 (368549495.464)\tLoss (L1) 2118.726 (2652.620)\n",
            "Val: [ 30/124]\tTime  0.004 ( 0.043)\tLoss (MSE) 70669048.000 (358231649.234)\tLoss (L1) 2880.513 (2759.603)\n",
            "Val: [ 40/124]\tTime  0.004 ( 0.033)\tLoss (MSE) 12956000.000 (278612938.884)\tLoss (L1) 1657.064 (2585.684)\n",
            "Val: [ 50/124]\tTime  0.002 ( 0.027)\tLoss (MSE) 102043184.000 (236998774.436)\tLoss (L1) 3794.167 (2559.373)\n",
            "Val: [ 60/124]\tTime  0.002 ( 0.024)\tLoss (MSE) 61229468.000 (216123073.750)\tLoss (L1) 2332.805 (2583.343)\n",
            "Val: [ 70/124]\tTime  0.002 ( 0.020)\tLoss (MSE) 702431488.000 (199015568.792)\tLoss (L1) 5417.814 (2554.618)\n",
            "Val: [ 80/124]\tTime  0.001 ( 0.018)\tLoss (MSE) 17790674.000 (209434457.806)\tLoss (L1) 1873.116 (2641.012)\n",
            "Val: [ 90/124]\tTime  0.002 ( 0.016)\tLoss (MSE) 61320672.000 (189529377.069)\tLoss (L1) 2142.893 (2570.875)\n",
            "Val: [100/124]\tTime  0.002 ( 0.015)\tLoss (MSE) 79490888.000 (175382540.903)\tLoss (L1) 3282.715 (2564.895)\n",
            "Val: [110/124]\tTime  0.004 ( 0.014)\tLoss (MSE) 6024353.000 (170636130.435)\tLoss (L1) 1272.731 (2559.659)\n",
            "Val: [120/124]\tTime  0.003 ( 0.013)\tLoss (MSE) 33511200.000 (160920066.904)\tLoss (L1) 2505.029 (2526.381)\n",
            " * Overall: MSE 158289899.740\tL1 2527.095\tG-Mean 649.585\n",
            " * Many: MSE 5568503.121\tL1 1323.057\tG-Mean 554.900\n",
            " * Median: MSE 88481493.937\tL1 4927.519\tG-Mean 1258.748\n",
            " * Low: MSE 5881136868.946\tL1 33457.132\tG-Mean 6994.274\n",
            "Best L1 Loss: 2526.534\n",
            "Epoch #9: Train loss [2362.8783]; Val loss: MSE [158289899.7402], L1 [2527.0949], G-Mean [649.5853]\n",
            "========================================================================================================================\n",
            "Test best model on testset...\n",
            "Loaded best model, epoch 8, best val loss 2526.5339\n",
            "Test: [  0/124]\tTime  1.170 ( 1.170)\tLoss (MSE) 53495864.000 (53495864.000)\tLoss (L1) 2170.488 (2170.488)\n",
            "Test: [ 10/124]\tTime  0.004 ( 0.110)\tLoss (MSE) 95311904.000 (35155970.636)\tLoss (L1) 3335.157 (2090.397)\n",
            "Test: [ 20/124]\tTime  0.005 ( 0.059)\tLoss (MSE) 20114796.000 (33490647.548)\tLoss (L1) 1742.370 (2015.050)\n",
            "Test: [ 30/124]\tTime  0.014 ( 0.042)\tLoss (MSE) 40273912.000 (34535682.823)\tLoss (L1) 2511.912 (2032.032)\n",
            "Test: [ 40/124]\tTime  0.001 ( 0.033)\tLoss (MSE) 146126752.000 (56452537.134)\tLoss (L1) 3043.086 (2205.291)\n",
            "Test: [ 50/124]\tTime  0.008 ( 0.027)\tLoss (MSE) 9351177.000 (56826435.775)\tLoss (L1) 1502.921 (2235.295)\n",
            "Test: [ 60/124]\tTime  0.001 ( 0.023)\tLoss (MSE) 676389952.000 (161879738.164)\tLoss (L1) 6172.160 (2476.465)\n",
            "Test: [ 70/124]\tTime  0.001 ( 0.020)\tLoss (MSE) 2272541.250 (148374933.708)\tLoss (L1) 897.261 (2456.069)\n",
            "Test: [ 80/124]\tTime  0.001 ( 0.018)\tLoss (MSE) 67435984.000 (133482296.349)\tLoss (L1) 3187.781 (2414.190)\n",
            "Test: [ 90/124]\tTime  0.001 ( 0.016)\tLoss (MSE) 10133300.000 (127419140.937)\tLoss (L1) 1544.811 (2427.245)\n",
            "Test: [100/124]\tTime  0.001 ( 0.015)\tLoss (MSE) 104908968.000 (120660646.458)\tLoss (L1) 3114.694 (2412.731)\n",
            "Test: [110/124]\tTime  0.001 ( 0.013)\tLoss (MSE) 8616383.000 (112376719.939)\tLoss (L1) 1719.249 (2392.289)\n",
            "Test: [120/124]\tTime  0.002 ( 0.013)\tLoss (MSE) 15363180.000 (107709688.083)\tLoss (L1) 2084.057 (2385.579)\n",
            " * Overall: MSE 107237215.078\tL1 2391.724\tG-Mean 721.478\n",
            " * Many: MSE 4985883.792\tL1 1309.469\tG-Mean 611.369\n",
            " * Median: MSE 73861052.421\tL1 4552.482\tG-Mean 1470.298\n",
            " * Low: MSE 4003562458.940\tL1 30480.166\tG-Mean 7220.287\n",
            "Test loss: MSE [107237215.0778], L1 [2391.7239], G-Mean [721.4782]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# train.py Part 2: Train/Evaluate the model.\n",
        "args.store_name = ''\n",
        "args.start_epoch, args.best_loss = 0, 1e5\n",
        "\n",
        "if len(args.store_name):\n",
        "    args.store_name = f'_{args.store_name}'\n",
        "if not args.lds and args.reweight != 'none':\n",
        "    args.store_name += f'_{args.reweight}'\n",
        "if args.lds:\n",
        "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
        "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
        "        args.store_name += f'_{args.lds_sigma}'\n",
        "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
        "\n",
        "prepare_folders(args)\n",
        "\n",
        "print(f\"Args: {args}\")\n",
        "print(f\"Store name: {args.store_name}\")\n",
        "\n",
        "def main():\n",
        "    if args.gpu is not None:\n",
        "        print(f\"Use GPU: {args.gpu} for training\")\n",
        "\n",
        "    # Data\n",
        "    print('=====> Preparing data...')\n",
        "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
        "\n",
        "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
        "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
        "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
        "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
        "                             num_workers=args.workers, pin_memory=True, drop_last=False)\n",
        "    print(f\"Training data size: {len(train_dataset)}\")\n",
        "    print(f\"Validation data size: {len(val_dataset)}\")\n",
        "    print(f\"Test data size: {len(test_dataset)}\")\n",
        "\n",
        "    # Random Seed\n",
        "    np.random.seed(999)\n",
        "    # random.seed(999)\n",
        "    torch.manual_seed(999)\n",
        "\n",
        "    # Model\n",
        "    print('=====> Building model...')\n",
        "    model = fcnet1()\n",
        "    if not args.cpu_only:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # evaluate only\n",
        "    if args.evaluate:\n",
        "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
        "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
        "        return\n",
        "\n",
        "    # Loss and optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
        "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
        "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
        "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            args.best_loss = checkpoint['best_loss']\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
        "        else:\n",
        "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
        "\n",
        "    if not args.cpu_only:\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epoch):\n",
        "        adjust_learning_rate(optimizer, epoch, args)\n",
        "        train_loss = train(train_loader, model, optimizer, epoch)\n",
        "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
        "\n",
        "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
        "        is_best = loss_metric < args.best_loss\n",
        "        args.best_loss = min(loss_metric, args.best_loss)\n",
        "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
        "        save_checkpoint(args, {\n",
        "            'epoch': epoch + 1,\n",
        "            'model': args.model,\n",
        "            'best_loss': args.best_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best)\n",
        "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
        "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
        "\n",
        "    # test with best checkpoint\n",
        "    print(\"=\" * 120)\n",
        "    print(\"Test best model on testset...\")\n",
        "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
        "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
        "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch):\n",
        "    batch_time = AverageMeter('Time', ':6.2f')\n",
        "    data_time = AverageMeter('Data', ':6.4f')\n",
        "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch)\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "        if not args.cpu_only:\n",
        "            inputs, targets, weights = \\\n",
        "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
        "        outputs = model(inputs, targets, epoch)\n",
        "\n",
        "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
        "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
        "\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if idx % args.print_freq == 0:\n",
        "            progress.display(idx)\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
        "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [batch_time, losses_mse, losses_l1],\n",
        "        prefix=f'{prefix}: '\n",
        "    )\n",
        "\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "    criterion_gmean = nn.L1Loss(reduction='none')\n",
        "\n",
        "    model.eval()\n",
        "    losses_all = []\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
        "            if not args.cpu_only:\n",
        "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            preds.extend(outputs.data.cpu().numpy())\n",
        "            labels.extend(targets.data.cpu().numpy())\n",
        "\n",
        "            loss_mse = criterion_mse(outputs, targets)\n",
        "            loss_l1 = criterion_l1(outputs, targets)\n",
        "            loss_all = criterion_gmean(outputs, targets)\n",
        "            losses_all.extend(loss_all.cpu().numpy())\n",
        "\n",
        "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
        "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if idx % args.print_freq == 0:\n",
        "                progress.display(idx)\n",
        "\n",
        "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
        "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
        "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
        "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
        "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
        "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
        "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
        "\n",
        "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
        "\n",
        "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
        "    train_labels = np.array(train_labels).astype(int)\n",
        "\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "    elif isinstance(preds, np.ndarray):\n",
        "        pass\n",
        "    else:\n",
        "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
        "\n",
        "    labels = np.array(labels).astype(int)\n",
        "\n",
        "    train_class_count, test_class_count = [], []\n",
        "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
        "    for l in np.unique(labels):\n",
        "        train_class_count.append(len(train_labels[train_labels == l]))\n",
        "        test_class_count.append(len(labels[labels == l]))\n",
        "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
        "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
        "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
        "\n",
        "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
        "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
        "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
        "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
        "\n",
        "    for i in range(len(train_class_count)):\n",
        "        if train_class_count[i] > many_shot_thr:\n",
        "            many_shot_mse.append(mse_per_class[i])\n",
        "            many_shot_l1.append(l1_per_class[i])\n",
        "            many_shot_gmean += list(l1_all_per_class[i])\n",
        "            many_shot_cnt.append(test_class_count[i])\n",
        "        elif train_class_count[i] < low_shot_thr:\n",
        "            low_shot_mse.append(mse_per_class[i])\n",
        "            low_shot_l1.append(l1_per_class[i])\n",
        "            low_shot_gmean += list(l1_all_per_class[i])\n",
        "            low_shot_cnt.append(test_class_count[i])\n",
        "        else:\n",
        "            median_shot_mse.append(mse_per_class[i])\n",
        "            median_shot_l1.append(l1_per_class[i])\n",
        "            median_shot_gmean += list(l1_all_per_class[i])\n",
        "            median_shot_cnt.append(test_class_count[i])\n",
        "\n",
        "    shot_dict = defaultdict(dict)\n",
        "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
        "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
        "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
        "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
        "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
        "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
        "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
        "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
        "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
        "\n",
        "    return shot_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zgVNKgFin9cx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_imbalance_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}